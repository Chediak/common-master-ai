{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chediak/common-master-ai/blob/main/prodesan_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y1cGJfDVKHzU"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber sentence-transformers faiss-cpu spacy requests elasticsearch requests\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install fastapi uvicorn\n",
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MImgYBmWv0WO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4v3uaYf4KIxT"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [page.extract_text() for page in pdf.pages]\n",
        "    return pages\n",
        "\n",
        "def preprocess_and_split_text(pages):\n",
        "    news_items = [item.strip() for page in pages for item in page.split(\"\\n\\n\") if item.strip()]\n",
        "    return news_items\n",
        "\n",
        "def generate_embeddings(news_items, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(news_items)\n",
        "    return embeddings, model\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def add_metadata(news_items):\n",
        "    metadata = [{\"id\": i, \"content\": news_items[i], \"length\": len(news_items[i])} for i in range(len(news_items))]\n",
        "    return metadata\n",
        "\n",
        "def extract_entities(news_items):\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "    extracted_data = []\n",
        "    for idx, news in enumerate(news_items):\n",
        "        doc = nlp(news)\n",
        "        entities = {\"id\": idx, \"content\": news, \"dates\": [], \"names\": [], \"organizations\": []}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"DATE\":\n",
        "                entities[\"dates\"].append(ent.text)\n",
        "            elif ent.label_ == \"PERSON\":\n",
        "                entities[\"names\"].append(ent.text)\n",
        "            elif ent.label_ == \"ORG\":\n",
        "                entities[\"organizations\"].append(ent.text)\n",
        "\n",
        "        cnpj_matches = re.findall(r\"\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}\", news)\n",
        "        entities[\"cnpjs\"] = cnpj_matches\n",
        "        extracted_data.append(entities)\n",
        "    return extracted_data\n",
        "\n",
        "def create_structured_index(entities):\n",
        "    index = {\"dates\": {}, \"names\": {}, \"organizations\": {}, \"cnpjs\": {}}\n",
        "    for item in entities:\n",
        "        for date in item[\"dates\"]:\n",
        "            index[\"dates\"].setdefault(date, []).append(item)\n",
        "        for name in item[\"names\"]:\n",
        "            index[\"names\"].setdefault(name, []).append(item)\n",
        "        for org in item[\"organizations\"]:\n",
        "            index[\"organizations\"].setdefault(org, []).append(item)\n",
        "        for cnpj in item[\"cnpjs\"]:\n",
        "            index[\"cnpjs\"].setdefault(cnpj, []).append(item)\n",
        "    return index\n",
        "\n",
        "def query_index(index, query_type, query_value):\n",
        "    if query_type in index:\n",
        "        return index[query_type].get(query_value, [])\n",
        "    return []\n",
        "\n",
        "def optimize_layout(metadata, top_k=5):\n",
        "    sorted_news = sorted(metadata, key=lambda x: x[\"length\"], reverse=True)\n",
        "    layout = []\n",
        "    for i, item in enumerate(sorted_news):\n",
        "        layout.append({\n",
        "            \"type\": \"news\",\n",
        "            \"content\": item[\"content\"],\n",
        "            \"page\": i // top_k + 1,\n",
        "            \"position\": i % top_k\n",
        "        })\n",
        "        if (i + 1) % 3 == 0:\n",
        "            layout.append({\n",
        "                \"type\": \"advertisement\",\n",
        "                \"content\": \"Ad Placeholder\",\n",
        "                \"page\": (i + 1) // top_k + 1,\n",
        "                \"position\": \"bottom\"\n",
        "            })\n",
        "    return layout\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"/content/RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Preprocessing and splitting text...\")\n",
        "    news_items = preprocess_and_split_text(pages)\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings, model = generate_embeddings(news_items)\n",
        "\n",
        "    print(\"Creating FAISS index...\")\n",
        "    index = create_faiss_index(np.array(embeddings))\n",
        "\n",
        "    print(\"Adding metadata...\")\n",
        "    metadata = add_metadata(news_items)\n",
        "\n",
        "    print(\"Extracting entities...\")\n",
        "    entities = extract_entities(news_items)\n",
        "\n",
        "    print(\"Creating structured index...\")\n",
        "    structured_index = create_structured_index(entities)\n",
        "\n",
        "    print(\"Querying by date '13/11/2024'...\")\n",
        "    date_results = query_index(structured_index, \"dates\", \"13/11/2024\")\n",
        "    print(json.dumps(date_results, indent=2))\n",
        "\n",
        "    print(\"Querying by organization 'ARAUJO E REPLANDE LTDA'...\")\n",
        "    org_results = query_index(structured_index, \"organizations\", \"ARAUJO E REPLANDE LTDA\")\n",
        "    print(json.dumps(org_results, indent=2))\n",
        "\n",
        "    print(\"Optimizing layout...\")\n",
        "    layout = optimize_layout(metadata)\n",
        "    print(json.dumps(layout, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AUVPbhFqKLqS"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [page.extract_text() for page in pdf.pages]\n",
        "    return pages\n",
        "\n",
        "def preprocess_and_split_text(pages):\n",
        "    news_items = [item.strip() for page in pages for item in page.split(\"\\n\\n\") if item.strip()]\n",
        "    return news_items\n",
        "\n",
        "def generate_embeddings(news_items, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(news_items)\n",
        "    return embeddings, model\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def add_metadata(news_items):\n",
        "    metadata = [{\"id\": i, \"content\": news_items[i], \"length\": len(news_items[i])} for i in range(len(news_items))]\n",
        "    return metadata\n",
        "\n",
        "def search_similar_news(query, model, index, metadata, top_k=5):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    results = [\n",
        "        {\"content\": metadata[idx][\"content\"], \"distance\": float(dist), \"length\": metadata[idx][\"length\"]}\n",
        "        for dist, idx in zip(distances[0], indices[0])\n",
        "    ]\n",
        "    return results\n",
        "\n",
        "def optimize_layout(metadata, top_k=5):\n",
        "    sorted_news = sorted(metadata, key=lambda x: x[\"length\"], reverse=True)\n",
        "\n",
        "    layout = []\n",
        "    for i, item in enumerate(sorted_news):\n",
        "        layout.append({\"type\": \"news\", \"content\": item[\"content\"], \"page\": i // top_k + 1, \"position\": i % top_k})\n",
        "        if (i + 1) % 3 == 0:\n",
        "            layout.append({\"type\": \"advertisement\", \"content\": \"Ad Placeholder\", \"page\": (i + 1) // top_k + 1, \"position\": \"bottom\"})\n",
        "\n",
        "    return layout\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"/content/RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Preprocessing and splitting text...\")\n",
        "    news_items = preprocess_and_split_text(pages)\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings, model = generate_embeddings(news_items)\n",
        "\n",
        "    print(\"Creating FAISS index...\")\n",
        "    index = create_faiss_index(np.array(embeddings))\n",
        "\n",
        "    print(\"Adding metadata...\")\n",
        "    metadata = add_metadata(news_items)\n",
        "\n",
        "    query = \"example query about a topic\"\n",
        "    print(\"Searching for similar news...\")\n",
        "    results = search_similar_news(query, model, index, metadata, top_k=5)\n",
        "\n",
        "    print(\"Search Results:\")\n",
        "    print(json.dumps(results, indent=2))\n",
        "\n",
        "    print(\"Optimizing layout...\")\n",
        "    layout = optimize_layout(metadata)\n",
        "\n",
        "    print(\"Layout:\")\n",
        "    print(json.dumps(layout, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1xuCgG-nKNcb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "input_data = {\n",
        "    \"pdf_data\": [\n",
        "        {\"id\": \"f97fb8c9-60d6-4fed-a82d-1cdf4e8be0bc\", \"altura\": 125, \"largura\": 35, \"agrupamento\": 1},\n",
        "        {\"id\": \"f97fb8c9-60d6-4fed-a82d-1cdf4e8be0bc\", \"altura\": 70, \"largura\": 40, \"agrupamento\": 2}\n",
        "    ]\n",
        "}\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def generate_output(input_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in input_data[\"pdf_data\"]:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "structured_output = generate_output(input_data)\n",
        "\n",
        "output_path = \"structured_output.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(structured_output, f, indent=2)\n",
        "\n",
        "print(f\"Structured JSON output saved to {output_path}\")\n",
        "structured_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k3T-zhObKOmX"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import json\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"largura\": 200,\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in pdf_data:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "pdf_path = \"/content/RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "print(\"Extracting data from the PDF...\")\n",
        "pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "print(\"Generating structured JSON output...\")\n",
        "structured_output = generate_output(pdf_data)\n",
        "\n",
        "output_path = \"structured_output.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(structured_output, f, indent=2)\n",
        "\n",
        "print(f\"Structured JSON output saved to {output_path}\")\n",
        "structured_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3qLmIfH8KP4S"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "import pdfplumber\n",
        "import json\n",
        "import os\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"largura\": 200,\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in pdf_data:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "@app.post(\"/upload-pdf/\")\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    pdf_path = f\"./{file.filename}\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    try:\n",
        "        pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "        structured_output = generate_output(pdf_data)\n",
        "\n",
        "        output_path = f\"./{os.path.splitext(file.filename)[0]}_output.json\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(structured_output, f, indent=2)\n",
        "\n",
        "        return JSONResponse(content=structured_output)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(pdf_path):\n",
        "            os.remove(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tKSyltt4KRNy"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "import pdfplumber\n",
        "import json\n",
        "import os\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420  #\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"largura\": 200,\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in pdf_data:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "@app.post(\"/upload-pdf/\")\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    pdf_path = f\"./{file.filename}\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    try:\n",
        "        pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "        structured_output = generate_output(pdf_data)\n",
        "\n",
        "        output_path = f\"./{os.path.splitext(file.filename)[0]}_output.json\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(structured_output, f, indent=2)\n",
        "\n",
        "        return JSONResponse(content=structured_output)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(pdf_path):\n",
        "            os.remove(pdf_path)\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"Starting FastAPI server...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1fsFekLiKSiR"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "\n",
        "# url = \"http://127.0.0.1:8000/upload-pdf/\"\n",
        "# files = {\"file\": open(\"RHOAI _ Prodesp - Diário Oficial.pdf\", \"rb\")}\n",
        "# response = requests.post(url, files=files)\n",
        "\n",
        "# print(response.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lkSLdTKCSjTJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import json\n",
        "\n",
        "MARGIN_TOP = 17\n",
        "MARGIN_BOTTOM = 13\n",
        "MARGIN_LEFT = 13\n",
        "MARGIN_RIGHT = 13\n",
        "MIN_SPACING_MM = 5\n",
        "PAGE_HEIGHT_MM = 420\n",
        "PAGE_WIDTH_MM = 297\n",
        "AD_SIZES = [(81, 100), (250, 210)]\n",
        "\n",
        "def calculate_cost(layout):\n",
        "    \"\"\"\n",
        "    Calcula o custo baseado em:\n",
        "    - Espaços vazios: Quanto menos espaço desperdiçado, menor o custo.\n",
        "    - Páginas usadas: Menos páginas, menor custo.\n",
        "    \"\"\"\n",
        "    unused_space = 0\n",
        "    total_pages = max(block['pagina'] for block in layout) + 1\n",
        "    for page in range(total_pages):\n",
        "        page_blocks = [b for b in layout if b['pagina'] == page]\n",
        "        used_space = sum(b['altura'] + MIN_SPACING_MM for b in page_blocks)\n",
        "        unused_space += PAGE_HEIGHT_MM - used_space\n",
        "    return total_pages * 100 + unused_space\n",
        "\n",
        "def generate_initial_layout(blocks):\n",
        "    layout = []\n",
        "    page = 0\n",
        "    current_y = MARGIN_TOP\n",
        "    for block in blocks:\n",
        "        if current_y + block['altura'] + MARGIN_BOTTOM > PAGE_HEIGHT_MM:\n",
        "            page += 1\n",
        "            current_y = MARGIN_TOP\n",
        "        layout.append({\n",
        "            \"id\": block['id'],\n",
        "            \"x\": MARGIN_LEFT,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": block.get('tipo', 'matéria'),\n",
        "            \"altura\": block['altura']\n",
        "        })\n",
        "        current_y += block['altura'] + MIN_SPACING_MM\n",
        "    return layout\n",
        "\n",
        "def perturb_layout(layout):\n",
        "    new_layout = layout[:]\n",
        "    idx1, idx2 = random.sample(range(len(new_layout)), 2)\n",
        "    new_layout[idx1], new_layout[idx2] = new_layout[idx2], new_layout[idx1]\n",
        "    return new_layout\n",
        "\n",
        "def simulated_annealing(blocks, initial_temp, cooling_rate, max_iterations):\n",
        "    current_layout = generate_initial_layout(blocks)\n",
        "    current_cost = calculate_cost(current_layout)\n",
        "    best_layout = current_layout[:]\n",
        "    best_cost = current_cost\n",
        "\n",
        "    temperature = initial_temp\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        new_layout = perturb_layout(current_layout)\n",
        "        new_cost = calculate_cost(new_layout)\n",
        "        delta = new_cost - current_cost\n",
        "\n",
        "        if delta < 0 or random.random() < math.exp(-delta / temperature):\n",
        "            current_layout = new_layout\n",
        "            current_cost = new_cost\n",
        "            if current_cost < best_cost:\n",
        "                best_layout = current_layout[:]\n",
        "                best_cost = current_cost\n",
        "\n",
        "        temperature *= cooling_rate\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteração {iteration}, Custo Atual: {current_cost}, Melhor Custo: {best_cost}\")\n",
        "\n",
        "    return best_layout, best_cost\n",
        "\n",
        "blocks = [\n",
        "    {\"id\": f\"block-{i}\", \"altura\": random.randint(50, 150), \"tipo\": \"matéria\"} for i in range(20)\n",
        "]\n",
        "\n",
        "initial_temp = 1000\n",
        "cooling_rate = 0.95\n",
        "max_iterations = 1000\n",
        "\n",
        "print(\"Executando Simulated Annealing para diagramação...\")\n",
        "optimized_layout, optimized_cost = simulated_annealing(blocks, initial_temp, cooling_rate, max_iterations)\n",
        "\n",
        "output = {\"pdf_data\": optimized_layout}\n",
        "output_path = \"optimized_layout.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(f\"Layout otimizado salvo em {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSDQuV3SWl68"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JTPzvgwUUHsk"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import json\n",
        "import re\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "MARGINS = {\"top\": 17, \"bottom\": 13, \"left\": 13, \"right\": 13}  # mm\n",
        "PAGE_HEIGHT_MM = 420\n",
        "MIN_SPACING_MM = 5\n",
        "AD_SIZES = [(81, 100), (250, 210)]\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "def extract_pdf_blocks(pdf_path):\n",
        "    \"\"\"Extract content blocks from the PDF.\"\"\"\n",
        "    extracted_blocks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_idx, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_blocks.append({\n",
        "                    \"id\": f\"block-{page_idx}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"pagina\": page_idx + 1\n",
        "                })\n",
        "    return extracted_blocks\n",
        "\n",
        "def auto_layout(blocks):\n",
        "    \"\"\"Generate layout JSON with alignment and ads.\"\"\"\n",
        "    layout = []\n",
        "    current_y = MARGINS[\"top\"]\n",
        "    page = 1\n",
        "\n",
        "    for block in blocks:\n",
        "        layout.append({\n",
        "            \"id\": block[\"id\"],\n",
        "            \"x\": MARGINS[\"left\"],\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": \"matéria\"\n",
        "        })\n",
        "        current_y += block[\"altura\"] + MIN_SPACING_MM\n",
        "        if current_y + MARGINS[\"bottom\"] > PAGE_HEIGHT_MM:\n",
        "            page += 1\n",
        "            current_y = MARGINS[\"top\"]\n",
        "\n",
        "        for ad_width, ad_height in AD_SIZES:\n",
        "            if current_y + ad_height + MARGINS[\"bottom\"] <= PAGE_HEIGHT_MM:\n",
        "                layout.append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": MARGINS[\"left\"],\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + MIN_SPACING_MM\n",
        "                break\n",
        "\n",
        "    return layout\n",
        "\n",
        "@app.post(\"/upload-pdf/\")\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    \"\"\"Upload PDF and return structured JSON.\"\"\"\n",
        "    pdf_path = f\"./{file.filename}\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    try:\n",
        "        blocks = extract_pdf_blocks(pdf_path)\n",
        "        structured_layout = auto_layout(blocks)\n",
        "        return JSONResponse(content={\"pdf_data\": structured_layout})\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "    finally:\n",
        "        os.remove(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MLXbWoe3lRj1"
      },
      "outputs": [],
      "source": [
        "from elasticsearch import Elasticsearch\n",
        "import pdfplumber\n",
        "import json\n",
        "\n",
        "ES_HOST = \"https://7d29aea5a554429db47e92ddc177f4d1.us-central1.gcp.cloud.es.io:443\"\n",
        "API_KEY = \"UURhZ1RwTUJIX19CdjAzZVJOUFY6X293dWg0RWtSbXVjSTVKYlVJb2pxQQ==\"\n",
        "\n",
        "es = Elasticsearch(\n",
        "    ES_HOST,\n",
        "    api_key=API_KEY\n",
        ")\n",
        "\n",
        "if es.ping():\n",
        "    print(\"Successfully connected to Elasticsearch\")\n",
        "else:\n",
        "    print(\"Failed to connect to Elasticsearch\")\n",
        "    raise RuntimeError(\"Elasticsearch connection failed.\")\n",
        "\n",
        "INDEX_NAME = \"pdf_index\"\n",
        "\n",
        "def create_index(index_name):\n",
        "    if not es.indices.exists(index=index_name):\n",
        "        es.indices.create(\n",
        "            index=index_name,\n",
        "            body={\n",
        "                \"mappings\": {\n",
        "                    \"properties\": {\n",
        "                        \"id\": {\"type\": \"keyword\"},\n",
        "                        \"content\": {\"type\": \"text\"},\n",
        "                        \"altura\": {\"type\": \"float\"},\n",
        "                        \"pagina\": {\"type\": \"integer\"},\n",
        "                        \"tipo\": {\"type\": \"keyword\"}\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        print(f\"Index '{index_name}' created successfully.\")\n",
        "    else:\n",
        "        print(f\"Index '{index_name}' already exists.\")\n",
        "\n",
        "create_index(INDEX_NAME)\n",
        "\n",
        "def extract_pdf_blocks(pdf_path):\n",
        "    extracted_blocks = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_idx, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_blocks.append({\n",
        "                    \"id\": f\"block-{page_idx}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"pagina\": page_idx + 1,\n",
        "                    \"tipo\": \"matéria\" if idx % 2 == 0 else \"titulo\"\n",
        "                })\n",
        "    return extracted_blocks\n",
        "\n",
        "def index_data(data, index_name):\n",
        "    for item in data:\n",
        "        es.index(index=index_name, id=item[\"id\"], body=item)\n",
        "    print(f\"{len(data)} documents indexed in Elasticsearch.\")\n",
        "\n",
        "pdf_path = \"/content/RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "print(\"Extracting data from PDF...\")\n",
        "blocks = extract_pdf_blocks(pdf_path)\n",
        "\n",
        "print(\"Indexing data into Elasticsearch...\")\n",
        "index_data(blocks, INDEX_NAME)\n",
        "\n",
        "def search_elasticsearch(query, index_name, field=\"content\", size=5):\n",
        "    response = es.search(\n",
        "        index=index_name,\n",
        "        body={\n",
        "            \"query\": {\n",
        "                \"match\": {\n",
        "                    field: query\n",
        "                }\n",
        "            },\n",
        "            \"size\": size\n",
        "        }\n",
        "    )\n",
        "    return response[\"hits\"][\"hits\"]\n",
        "\n",
        "print(\"Performing search in Elasticsearch...\")\n",
        "query = \"sua consulta aqui\"\n",
        "results = search_elasticsearch(query, INDEX_NAME)\n",
        "\n",
        "for result in results:\n",
        "    print(json.dumps(result[\"_source\"], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BnHbfDUGlW3y"
      },
      "outputs": [],
      "source": [
        "def search_elasticsearch(query, index_name, field=\"content\", size=5):\n",
        "    response = es.search(\n",
        "        index=index_name,\n",
        "        body={\n",
        "            \"query\": {\n",
        "                \"match\": {\n",
        "                    field: query\n",
        "                }\n",
        "            },\n",
        "            \"size\": size\n",
        "        }\n",
        "    )\n",
        "    return response[\"hits\"][\"hits\"]\n",
        "\n",
        "print(\"Buscando no Elasticsearch...\")\n",
        "query = \"oficial\"\n",
        "results = search_elasticsearch(query, INDEX_NAME)\n",
        "\n",
        "for result in results:\n",
        "    print(json.dumps(result[\"_source\"], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1-nWj7p9lhdl"
      },
      "outputs": [],
      "source": [
        "def advanced_search(query, index_name, page_filter=None, type_filter=None, size=5):\n",
        "    filters = []\n",
        "    if page_filter:\n",
        "        filters.append({\"term\": {\"pagina\": page_filter}})\n",
        "    if type_filter:\n",
        "        filters.append({\"term\": {\"tipo\": type_filter}})\n",
        "\n",
        "    body = {\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [{\"match\": {\"content\": query}}],\n",
        "                \"filter\": filters\n",
        "            }\n",
        "        },\n",
        "        \"size\": size\n",
        "    }\n",
        "\n",
        "    response = es.search(index=index_name, body=body)\n",
        "    return response[\"hits\"][\"hits\"]\n",
        "\n",
        "results = advanced_search(\"oficial\", INDEX_NAME, page_filter=1, type_filter=\"matéria\")\n",
        "for result in results:\n",
        "    print(json.dumps(result[\"_source\"], indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kQJbYxPG-lWn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def access_doe_api(endpoint, params=None):\n",
        "    url = f\"https://do-api-web-search.doe.sp.gov.br{endpoint}\"\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Erro {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "journals = access_doe_api(\"/v2/journals\")\n",
        "if journals:\n",
        "    print(json.dumps(journals, indent=2, ensure_ascii=False))\n",
        "\n",
        "journal_id = \"ca96256b-6ca1-407f-866e-567ef9430123\"\n",
        "sections = access_doe_api(f\"/v2/sections?JournalId={journal_id}\")\n",
        "if sections:\n",
        "    print(json.dumps(sections, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "28oSV0VA-qun"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"FromDate\": \"2024-11-21\",\n",
        "    \"ToDate\": \"2024-11-22\",\n",
        "    \"Terms[0]\": \"NOMEADO\",\n",
        "    \"Terms[1]\": \"EXTRADITADO\",\n",
        "    \"JournalId\": journal_id,\n",
        "    \"PageNumber\": 1,\n",
        "    \"PageSize\": 10\n",
        "}\n",
        "advanced_search = access_doe_api(\"/v2/advanced-search/publications\", params=params)\n",
        "if advanced_search:\n",
        "    print(json.dumps(advanced_search, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2drj1M0H-wDc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "BASE_URL = \"https://do-api-web-search.doe.sp.gov.br\"\n",
        "PDF_URL = \"https://www.imprensaoficial.com.br/downloads/pdf/edicao\"\n",
        "\n",
        "def make_request(endpoint, params=None):\n",
        "    response = requests.get(f\"{BASE_URL}{endpoint}\", params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Erro {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def listar_cadernos():\n",
        "    return make_request(\"/v2/journals\")\n",
        "\n",
        "def buscar_secoes_raiz(journal_id):\n",
        "    return make_request(f\"/v2/sections?JournalId={journal_id}\")\n",
        "\n",
        "def obter_hierarquia_dia(date, journal_id, section_id):\n",
        "    params = {\n",
        "        \"Date\": date,\n",
        "        \"JournalId\": journal_id,\n",
        "        \"SectionId\": section_id\n",
        "    }\n",
        "    return make_request(\"/v2/summary/structured\", params=params)\n",
        "\n",
        "def recuperar_materia(slug):\n",
        "    return make_request(f\"/v2/publications/{slug}\")\n",
        "\n",
        "def busca_avancada(from_date, to_date, terms, journal_id=None, section_id=None, page_number=1, page_size=20, sort_field=\"Date\"):\n",
        "    params = {\n",
        "        \"FromDate\": from_date,\n",
        "        \"ToDate\": to_date,\n",
        "        \"PageNumber\": page_number,\n",
        "        \"PageSize\": page_size,\n",
        "        \"SortField\": sort_field\n",
        "    }\n",
        "    for i, term in enumerate(terms):\n",
        "        params[f\"Terms[{i}]\"] = term\n",
        "    if journal_id:\n",
        "        params[\"JournalId\"] = journal_id\n",
        "    if section_id:\n",
        "        params[\"SectionId\"] = section_id\n",
        "    return make_request(\"/v2/advanced-search/publications\", params=params)\n",
        "\n",
        "def verificar_status_edicao(date):\n",
        "    response = requests.get(f\"https://do-api-publication-pdf.doe.sp.gov.br/v1/editions/status/{date}\")\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Erro {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def abrir_edicao_pdf(date, sigla):\n",
        "    url = f\"{PDF_URL}/{date}{sigla}.pdf\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        file_path = f\"{date}_{sigla}.pdf\"\n",
        "        with open(file_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"PDF salvo em {file_path}\")\n",
        "    else:\n",
        "        print(f\"Erro ao baixar PDF: {response.status_code}\")\n",
        "\n",
        "cadernos = listar_cadernos()\n",
        "print(\"Cadernos Disponíveis:\", json.dumps(cadernos, indent=2))\n",
        "\n",
        "if cadernos:\n",
        "    journal_id = cadernos[\"items\"][0][\"id\"]\n",
        "    secoes = buscar_secoes_raiz(journal_id)\n",
        "    print(\"Seções Raiz:\", json.dumps(secoes, indent=2))\n",
        "\n",
        "if secoes:\n",
        "    section_id = secoes[\"items\"][0][\"id\"]\n",
        "    hierarquia = obter_hierarquia_dia(\"2024-11-22\", journal_id, section_id)\n",
        "    print(\"Hierarquia do Dia:\", json.dumps(hierarquia, indent=2))\n",
        "\n",
        "if hierarquia:\n",
        "    if \"items\" in hierarquia and hierarquia[\"items\"]:\n",
        "        for item in hierarquia[\"items\"]:\n",
        "            if \"children\" in item and item[\"children\"]:\n",
        "                for child in item[\"children\"]:\n",
        "                    if \"publications\" in child and child[\"publications\"]:\n",
        "                        slug = child[\"publications\"][0][\"slug\"]\n",
        "                        materia = recuperar_materia(slug)\n",
        "                        print(\"Matéria Específica:\", json.dumps(materia, indent=2))\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"Nenhuma publicação encontrada no nível 'children'.\")\n",
        "            else:\n",
        "                print(\"Nenhuma 'children' encontrada no item da hierarquia.\")\n",
        "    else:\n",
        "        print(\"Nenhum item encontrado na hierarquia.\")\n",
        "\n",
        "\n",
        "busca = busca_avancada(\"2024-11-21\", \"2024-11-22\", [\"NOMEADO\", \"EXTRADITADO\"], journal_id)\n",
        "print(\"Busca Avançada:\", json.dumps(busca, indent=2))\n",
        "\n",
        "status_pdf = verificar_status_edicao(\"2024-11-22\")\n",
        "print(\"Status da Edição em PDF:\", json.dumps(status_pdf, indent=2))\n",
        "\n",
        "abrir_edicao_pdf(\"20241125\", \"EXEC1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oVqu-zCYIdcK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pdfplumber\n",
        "\n",
        "BASE_URL = \"https://do-api-web-search.doe.sp.gov.br\"\n",
        "PDF_URL = \"https://www.imprensaoficial.com.br/downloads/pdf/edicao\"\n",
        "\n",
        "MARGINS = {\"top\": 17, \"bottom\": 13, \"left\": 13, \"right\": 13}  # mm\n",
        "PAGE_HEIGHT_MM = 420\n",
        "MIN_SPACING_MM = 5\n",
        "AD_SIZES = [(81, 100), (250, 210)]\n",
        "\n",
        "def make_request(endpoint, params=None):\n",
        "    response = requests.get(f\"{BASE_URL}{endpoint}\", params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Erro {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def listar_cadernos():\n",
        "    return make_request(\"/v2/journals\")\n",
        "\n",
        "def buscar_secoes_raiz(journal_id):\n",
        "    return make_request(f\"/v2/sections?JournalId={journal_id}\")\n",
        "\n",
        "def obter_hierarquia_dia(date, journal_id, section_id):\n",
        "    params = {\n",
        "        \"Date\": date,\n",
        "        \"JournalId\": journal_id,\n",
        "        \"SectionId\": section_id\n",
        "    }\n",
        "    return make_request(\"/v2/summary/structured\", params=params)\n",
        "\n",
        "def recuperar_materia(slug):\n",
        "    return make_request(f\"/v2/publications/{slug}\")\n",
        "\n",
        "def busca_avancada(from_date, to_date, terms, journal_id=None, section_id=None, page_number=1, page_size=20, sort_field=\"Date\"):\n",
        "    params = {\n",
        "        \"FromDate\": from_date,\n",
        "        \"ToDate\": to_date,\n",
        "        \"PageNumber\": page_number,\n",
        "        \"PageSize\": page_size,\n",
        "        \"SortField\": sort_field\n",
        "    }\n",
        "    for i, term in enumerate(terms):\n",
        "        params[f\"Terms[{i}]\"] = term\n",
        "    if journal_id:\n",
        "        params[\"JournalId\"] = journal_id\n",
        "    if section_id:\n",
        "        params[\"SectionId\"] = section_id\n",
        "    return make_request(\"/v2/advanced-search/publications\", params=params)\n",
        "\n",
        "def prepare_diagram_data(api_results):\n",
        "    blocks = []\n",
        "    for result in api_results.get(\"publications\", []):\n",
        "        blocks.append({\n",
        "            \"id\": result[\"id\"],\n",
        "            \"content\": result[\"content\"],\n",
        "            \"altura\": len(result[\"content\"]) * 0.1,\n",
        "            \"tipo\": \"matéria\" if \"matéria\" in result.get(\"type\", \"\").lower() else \"título\"\n",
        "        })\n",
        "    return blocks\n",
        "\n",
        "def auto_layout(blocks):\n",
        "    layout = []\n",
        "    current_y = MARGINS[\"top\"]\n",
        "    page = 1\n",
        "\n",
        "    for block in blocks:\n",
        "        layout.append({\n",
        "            \"id\": block[\"id\"],\n",
        "            \"x\": MARGINS[\"left\"],\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": block[\"tipo\"]\n",
        "        })\n",
        "        current_y += block[\"altura\"] + MIN_SPACING_MM\n",
        "\n",
        "        if current_y + MARGINS[\"bottom\"] > PAGE_HEIGHT_MM:\n",
        "            page += 1\n",
        "            current_y = MARGINS[\"top\"]\n",
        "\n",
        "        for ad_width, ad_height in AD_SIZES:\n",
        "            if current_y + ad_height + MARGINS[\"bottom\"] <= PAGE_HEIGHT_MM:\n",
        "                layout.append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": MARGINS[\"left\"],\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + MIN_SPACING_MM\n",
        "                break\n",
        "\n",
        "    return layout\n",
        "\n",
        "def main():\n",
        "    busca_results = busca_avancada(\"2024-11-21\", \"2024-11-22\", [\"NOMEADO\", \"EXTRADITADO\"])\n",
        "    if not busca_results:\n",
        "        print(\"Nenhum resultado encontrado pela API.\")\n",
        "        return\n",
        "\n",
        "    print(\"Preparando dados para diagramação...\")\n",
        "    blocks = prepare_diagram_data(busca_results)\n",
        "\n",
        "    print(\"Gerando layout...\")\n",
        "    layout = auto_layout(blocks)\n",
        "\n",
        "    output_path = \"structured_layout.json\"\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump({\"pdf_data\": layout}, f, indent=2)\n",
        "    print(f\"Layout salvo em {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZVz_Uq2QOojz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DCa8EhA4Nv1m5WI_RvFKcXE0l3YHE0AW",
      "authorship_tag": "ABX9TyOdPKx2x+e+O0LSUlms9fG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}