{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJvZg95gMym/n3rzyeJ9hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chediak/common-master-ai/blob/main/prodesan_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1cGJfDVKHzU"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber sentence-transformers faiss-cpu spacy\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install fastapi uvicorn\n",
        "!pip install python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import spacy\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [page.extract_text() for page in pdf.pages]\n",
        "    return pages\n",
        "\n",
        "def preprocess_and_split_text(pages):\n",
        "    news_items = [item.strip() for page in pages for item in page.split(\"\\n\\n\") if item.strip()]\n",
        "    return news_items\n",
        "\n",
        "def generate_embeddings(news_items, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(news_items)\n",
        "    return embeddings, model\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def add_metadata(news_items):\n",
        "    metadata = [{\"id\": i, \"content\": news_items[i], \"length\": len(news_items[i])} for i in range(len(news_items))]\n",
        "    return metadata\n",
        "\n",
        "def extract_entities(news_items):\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "    extracted_data = []\n",
        "    for idx, news in enumerate(news_items):\n",
        "        doc = nlp(news)\n",
        "        entities = {\"id\": idx, \"content\": news, \"dates\": [], \"names\": [], \"organizations\": []}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"DATE\":\n",
        "                entities[\"dates\"].append(ent.text)\n",
        "            elif ent.label_ == \"PERSON\":\n",
        "                entities[\"names\"].append(ent.text)\n",
        "            elif ent.label_ == \"ORG\":\n",
        "                entities[\"organizations\"].append(ent.text)\n",
        "\n",
        "        cnpj_matches = re.findall(r\"\\d{2}\\.\\d{3}\\.\\d{3}/\\d{4}-\\d{2}\", news)\n",
        "        entities[\"cnpjs\"] = cnpj_matches\n",
        "        extracted_data.append(entities)\n",
        "    return extracted_data\n",
        "\n",
        "def create_structured_index(entities):\n",
        "    index = {\"dates\": {}, \"names\": {}, \"organizations\": {}, \"cnpjs\": {}}\n",
        "    for item in entities:\n",
        "        for date in item[\"dates\"]:\n",
        "            index[\"dates\"].setdefault(date, []).append(item)\n",
        "        for name in item[\"names\"]:\n",
        "            index[\"names\"].setdefault(name, []).append(item)\n",
        "        for org in item[\"organizations\"]:\n",
        "            index[\"organizations\"].setdefault(org, []).append(item)\n",
        "        for cnpj in item[\"cnpjs\"]:\n",
        "            index[\"cnpjs\"].setdefault(cnpj, []).append(item)\n",
        "    return index\n",
        "\n",
        "def query_index(index, query_type, query_value):\n",
        "    if query_type in index:\n",
        "        return index[query_type].get(query_value, [])\n",
        "    return []\n",
        "\n",
        "def optimize_layout(metadata, top_k=5):\n",
        "    sorted_news = sorted(metadata, key=lambda x: x[\"length\"], reverse=True)\n",
        "    layout = []\n",
        "    for i, item in enumerate(sorted_news):\n",
        "        layout.append({\n",
        "            \"type\": \"news\",\n",
        "            \"content\": item[\"content\"],\n",
        "            \"page\": i // top_k + 1,\n",
        "            \"position\": i % top_k\n",
        "        })\n",
        "        if (i + 1) % 3 == 0:\n",
        "            layout.append({\n",
        "                \"type\": \"advertisement\",\n",
        "                \"content\": \"Ad Placeholder\",\n",
        "                \"page\": (i + 1) // top_k + 1,\n",
        "                \"position\": \"bottom\"\n",
        "            })\n",
        "    return layout\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Preprocessing and splitting text...\")\n",
        "    news_items = preprocess_and_split_text(pages)\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings, model = generate_embeddings(news_items)\n",
        "\n",
        "    print(\"Creating FAISS index...\")\n",
        "    index = create_faiss_index(np.array(embeddings))\n",
        "\n",
        "    print(\"Adding metadata...\")\n",
        "    metadata = add_metadata(news_items)\n",
        "\n",
        "    print(\"Extracting entities...\")\n",
        "    entities = extract_entities(news_items)\n",
        "\n",
        "    print(\"Creating structured index...\")\n",
        "    structured_index = create_structured_index(entities)\n",
        "\n",
        "    print(\"Querying by date '13/11/2024'...\")\n",
        "    date_results = query_index(structured_index, \"dates\", \"13/11/2024\")\n",
        "    print(json.dumps(date_results, indent=2))\n",
        "\n",
        "    print(\"Querying by organization 'ARAUJO E REPLANDE LTDA'...\")\n",
        "    org_results = query_index(structured_index, \"organizations\", \"ARAUJO E REPLANDE LTDA\")\n",
        "    print(json.dumps(org_results, indent=2))\n",
        "\n",
        "    print(\"Optimizing layout...\")\n",
        "    layout = optimize_layout(metadata)\n",
        "    print(json.dumps(layout, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "4v3uaYf4KIxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        pages = [page.extract_text() for page in pdf.pages]\n",
        "    return pages\n",
        "\n",
        "def preprocess_and_split_text(pages):\n",
        "    news_items = [item.strip() for page in pages for item in page.split(\"\\n\\n\") if item.strip()]\n",
        "    return news_items\n",
        "\n",
        "def generate_embeddings(news_items, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(news_items)\n",
        "    return embeddings, model\n",
        "\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def add_metadata(news_items):\n",
        "    metadata = [{\"id\": i, \"content\": news_items[i], \"length\": len(news_items[i])} for i in range(len(news_items))]\n",
        "    return metadata\n",
        "\n",
        "def search_similar_news(query, model, index, metadata, top_k=5):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    results = [\n",
        "        {\"content\": metadata[idx][\"content\"], \"distance\": float(dist), \"length\": metadata[idx][\"length\"]}\n",
        "        for dist, idx in zip(distances[0], indices[0])\n",
        "    ]\n",
        "    return results\n",
        "\n",
        "def optimize_layout(metadata, top_k=5):\n",
        "    # Sort news items by length (longer news first)\n",
        "    sorted_news = sorted(metadata, key=lambda x: x[\"length\"], reverse=True)\n",
        "\n",
        "    layout = []\n",
        "    for i, item in enumerate(sorted_news):\n",
        "        layout.append({\"type\": \"news\", \"content\": item[\"content\"], \"page\": i // top_k + 1, \"position\": i % top_k})\n",
        "        if (i + 1) % 3 == 0:\n",
        "            layout.append({\"type\": \"advertisement\", \"content\": \"Ad Placeholder\", \"page\": (i + 1) // top_k + 1, \"position\": \"bottom\"})\n",
        "\n",
        "    return layout\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "    print(\"Extracting text from PDF...\")\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"Preprocessing and splitting text...\")\n",
        "    news_items = preprocess_and_split_text(pages)\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    embeddings, model = generate_embeddings(news_items)\n",
        "\n",
        "    print(\"Creating FAISS index...\")\n",
        "    index = create_faiss_index(np.array(embeddings))\n",
        "\n",
        "    print(\"Adding metadata...\")\n",
        "    metadata = add_metadata(news_items)\n",
        "\n",
        "    query = \"example query about a topic\"\n",
        "    print(\"Searching for similar news...\")\n",
        "    results = search_similar_news(query, model, index, metadata, top_k=5)\n",
        "\n",
        "    print(\"Search Results:\")\n",
        "    print(json.dumps(results, indent=2))\n",
        "\n",
        "    print(\"Optimizing layout...\")\n",
        "    layout = optimize_layout(metadata)\n",
        "\n",
        "    print(\"Layout:\")\n",
        "    print(json.dumps(layout, indent=2))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AUVPbhFqKLqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_data = {\n",
        "    \"pdf_data\": [\n",
        "        {\"id\": \"f97fb8c9-60d6-4fed-a82d-1cdf4e8be0bc\", \"altura\": 125, \"largura\": 35, \"agrupamento\": 1},\n",
        "        {\"id\": \"f97fb8c9-60d6-4fed-a82d-1cdf4e8be0bc\", \"altura\": 70, \"largura\": 40, \"agrupamento\": 2}\n",
        "    ]\n",
        "}\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def generate_output(input_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in input_data[\"pdf_data\"]:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "structured_output = generate_output(input_data)\n",
        "\n",
        "output_path = \"structured_output.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(structured_output, f, indent=2)\n",
        "\n",
        "print(f\"Structured JSON output saved to {output_path}\")\n",
        "structured_output\n"
      ],
      "metadata": {
        "id": "1xuCgG-nKNcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import json\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            # Split text into blocks for processing\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"largura\": 200,\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in pdf_data:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "pdf_path = \"RHOAI _ Prodesp - Diário Oficial.pdf\"\n",
        "\n",
        "print(\"Extracting data from the PDF...\")\n",
        "pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "print(\"Generating structured JSON output...\")\n",
        "structured_output = generate_output(pdf_data)\n",
        "\n",
        "output_path = \"structured_output.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(structured_output, f, indent=2)\n",
        "\n",
        "print(f\"Structured JSON output saved to {output_path}\")\n",
        "structured_output\n"
      ],
      "metadata": {
        "id": "k3T-zhObKOmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "import pdfplumber\n",
        "import json\n",
        "import os\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420\n",
        "ad_sizes = [(81, 100), (250, 210)]\n",
        "\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,\n",
        "                    \"largura\": 200,\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top\n",
        "\n",
        "    for item in pdf_data:\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "@app.post(\"/upload-pdf/\")\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    pdf_path = f\"./{file.filename}\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    try:\n",
        "        pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "        structured_output = generate_output(pdf_data)\n",
        "\n",
        "        output_path = f\"./{os.path.splitext(file.filename)[0]}_output.json\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(structured_output, f, indent=2)\n",
        "\n",
        "        return JSONResponse(content=structured_output)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(pdf_path):\n",
        "            os.remove(pdf_path)\n",
        "\n",
        "# To run the API server:\n",
        "# Use the command: uvicorn <filename>:app --reload\n"
      ],
      "metadata": {
        "id": "3qLmIfH8KP4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "import pdfplumber\n",
        "import json\n",
        "import os\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Constants for layout rules\n",
        "margin_top = 17  # mm\n",
        "margin_bottom = 13  # mm\n",
        "margin_left = 13  # mm\n",
        "margin_right = 13  # mm\n",
        "min_spacing_mm = 5  # mm\n",
        "page_height_mm = 420  # Approx height of A3 in mm\n",
        "ad_sizes = [(81, 100), (250, 210)]  # Example sizes in mm\n",
        "\n",
        "# Extract text and preprocess data from PDF\n",
        "def extract_pdf_data(pdf_path):\n",
        "    extracted_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number, page in enumerate(pdf.pages, start=1):\n",
        "            text = page.extract_text()\n",
        "            if not text:\n",
        "                continue\n",
        "            # Split text into blocks for processing\n",
        "            for idx, block in enumerate(text.split(\"\\n\\n\")):\n",
        "                extracted_data.append({\n",
        "                    \"id\": f\"block-{page_number}-{idx}\",\n",
        "                    \"content\": block.strip(),\n",
        "                    \"altura\": len(block) * 0.1,  # Mock height based on content length\n",
        "                    \"largura\": 200,  # Arbitrary width\n",
        "                    \"agrupamento\": 1 if idx % 2 == 0 else 2  # Alternate grouping\n",
        "                })\n",
        "    return extracted_data\n",
        "\n",
        "# Generate the expected JSON output with alignment and spacing rules\n",
        "def generate_output(pdf_data):\n",
        "    output_data = {\"pdf_data\": []}\n",
        "    page = 1\n",
        "    current_y = margin_top  # Start at the top margin\n",
        "\n",
        "    for item in pdf_data:\n",
        "        # Add an entry for the main content\n",
        "        content_type = \"matéria\" if item[\"agrupamento\"] == 1 else \"titulo\"\n",
        "        output_data[\"pdf_data\"].append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"x\": margin_left,  # Always align to left margin\n",
        "            \"y\": current_y,\n",
        "            \"pagina\": page,\n",
        "            \"tipo\": content_type\n",
        "        })\n",
        "        # Update Y position considering the height and spacing\n",
        "        current_y += item[\"altura\"] + min_spacing_mm\n",
        "\n",
        "        # Check if a new page is needed\n",
        "        if current_y + margin_bottom > page_height_mm:\n",
        "            page += 1\n",
        "            current_y = margin_top\n",
        "\n",
        "        # Add calhau (ad placeholder) if space permits\n",
        "        for ad_width, ad_height in ad_sizes:\n",
        "            if current_y + ad_height + margin_bottom <= page_height_mm:\n",
        "                output_data[\"pdf_data\"].append({\n",
        "                    \"id\": \"calhau-placeholder\",\n",
        "                    \"x\": margin_left,\n",
        "                    \"y\": current_y,\n",
        "                    \"pagina\": page,\n",
        "                    \"tipo\": f\"calhau-{ad_width}x{ad_height}\"\n",
        "                })\n",
        "                current_y += ad_height + min_spacing_mm\n",
        "                break\n",
        "\n",
        "    return output_data\n",
        "\n",
        "@app.post(\"/upload-pdf/\")\n",
        "async def upload_pdf(file: UploadFile = File(...)):\n",
        "    # Save the uploaded file\n",
        "    pdf_path = f\"./{file.filename}\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "    try:\n",
        "        # Extract data from the PDF\n",
        "        pdf_data = extract_pdf_data(pdf_path)\n",
        "\n",
        "        # Generate the structured output\n",
        "        structured_output = generate_output(pdf_data)\n",
        "\n",
        "        # Save the structured JSON output\n",
        "        output_path = f\"./{os.path.splitext(file.filename)[0]}_output.json\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(structured_output, f, indent=2)\n",
        "\n",
        "        # Return the structured output as response\n",
        "        return JSONResponse(content=structured_output)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    finally:\n",
        "        # Clean up: Remove the uploaded file\n",
        "        if os.path.exists(pdf_path):\n",
        "            os.remove(pdf_path)\n",
        "\n",
        "# Run the server in Jupyter Notebook\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()  # Allow nested event loops in Jupyter\n",
        "\n",
        "# Start the FastAPI server\n",
        "print(\"Starting FastAPI server...\")\n",
        "uvicorn.run(app, host=\"127.0.0.1\", port=8000)"
      ],
      "metadata": {
        "id": "tKSyltt4KRNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://127.0.0.1:8000/upload-pdf/\"\n",
        "files = {\"file\": open(\"RHOAI _ Prodesp - Diário Oficial.pdf\", \"rb\")}\n",
        "response = requests.post(url, files=files)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "1fsFekLiKSiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}